<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
        content="A short experiment to investigate and demonstrate the science behind tracking how people walk">
    <meta name="author" content="Raphael Leung">
    <link rel="icon" href="assets/favicon.ico">
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date(); a = s.createElement(o),
                m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-42686900-2', 'auto');
        ga('send', 'pageview');
    </script>

    <title>Silly walking - taken seriously</title>

    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/clean-blog.min.css" rel="stylesheet">

    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet'
        type='text/css'>
    <link
        href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800'
        rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" media="screen"
        href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/1.3.4/jquery.fancybox-1.3.4.css" />
    <style type="text/css">
        .center-silly {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 80%;
            /* 20vw; */
        }
    </style>

</head>

<body>

    <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="https://code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/1.3.4/jquery.fancybox-1.3.4.pack.min.js"></script>

    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <div class="navbar-header page-scroll">
                <a class="navbar-brand" href="index.html" style="font-size: 12px;">Home page</a>
                <!-- <a class="navbar-brand" href="blog.html">Blog</a> -->
            </div>


        </div>
    </nav>

    <header class="intro-header" style="background-image: url('assets/silly_walks/demo_compressed.gif')">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="page-heading">
                        <h1 style="margin-top: 400px; font-size: 46px">Silly walking - taken seriously</h1>
                        <!-- <hr class="small"> -->
                    </div>
                </div>
            </div>
        </div>
    </header>

    <article>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">

                    <p style="font-size: 16px">by <a href="https://www.nesta.org.uk/team/raphael-leung/" target="_blank">Raphael Leung</a></p>

                    <p>
                        One of Nesta’s annual predictions made in December 2019 was an <a
                            href="https://www.nesta.org.uk/feature/ten-predictions-2020/outbreak-monty-python-style-silly-walks/"
                            target="_blank">outbreak
                            of Monty Python-style
                            silly walks</a> to counter gait identification, or surveillance that identifies people based
                        on how someone walks. Nesta's Explorations Initiatives funded my short experiment to
                        investigate and demonstrate the science behind tracking how people walk.
                    </p>
                    <p>
                        Monty Python’s "Ministry of Silly Walks" was a satire on bureaucratic inefficiency, first aired
                        on the BBC in 1970. Unless you work at the Ministry of Silly Walks, silly walking is not meant
                        to be taken seriously. But is it possible to measure a silly walk, and
                        practically how would we go about doing it?
                    </p>

                    <figure>
                        <img src='assets/silly_walks/yorkshiresillywalks.PNG' alt='Yorkshire Silly Walks'
                            class="center-silly" />
                        <figcaption style="font-size: 16px; text-align: center;" class="center-silly"><a
                                href="https://www.instagram.com/yorkshire.silly.walks/" target="_blank">Yorkshire Silly
                                Walks</a> was one of a handful of silly-walking neighbourhoods in the UK
                            that started during the Covid pandemic lockdown.
                        </figcaption>
                    </figure>
                    <p>I tested how off-the-shelf software and hardware can be used to accurately measure and detect
                        silly walks. The goal is to encourage readers to re-engage with silly walks in a fun and new
                        way, and realise the trade-offs
                        one makes when reckoning with emerging technologies.
                    </p>


                    <h2 class="section-heading">Roots of motion capture</h2>

                    <p>While we may think motion capture is a relatively recent innovation -- maybe associating it with
                        green suits and visual effects from our favourite films -- the principles of motion capture can
                        be traced much further back.
                    </p>

                    <p> In 1878, photographer Eadweard Muybridge set up <a
                            href="https://spectrum.ieee.org/consumer-electronics/audiovideo/june-1878-muybridge-photographs-a-galloping-horse"
                            target="_blank">
                            an experiment</a>: using thin threads that easily broke to trigger camera shutters
                        (to settle an argument on whether all hooves go off the ground
                        when a horse gallops). Later, French doctor Étienne-Jules Marey invented a
                        ‘photographic gun’ that captures a range of movement
                        by people and animals. So the practice of
                        measuring ourselves in motion is really not new at all. We just have increasingly interesting
                        ways of, and reasons for, doing it.
                    </p>

                    <figure>
                        <img src='assets/silly_walks/highjump.jpg'
                            alt='Phases of movement of a man doing high jump, Étienne-Jules Marey' class="center-silly"
                            width="250px" />
                        <figcaption style="font-size: 16px; text-align: center;" class="center-silly">Phases of movement
                            of a man doing high jump, Étienne-Jules Marey (1892). Photo taken at the Science Musuem "<a
                                href="https://www.sciencemuseum.org.uk/what-was-on/art-innovation-enlightenment-dark-matter"
                                target="_blank">Art of innovation</a>" exhibition.

                        </figcaption>
                    </figure>

                    <h2 class="section-heading">Measuring silly walks</h2>

                    <p>Next, I apply a data science lens to the very serious topic of silly walking, using a method
                        called pose estimation. It estimates where key body joints are in images and videos, analogous
                        to drawing stick figures over limbs. Accuracy varies
                        by filming conditions and parameters: there may be false positives due to clutter or false
                        negatives due to occlusion. On its own, pose estimation does not recognise or identify
                        people.
                    </p>
                    <p>
                        I tried integrating various tools for skeletal tracking and
                        pose estimation. These include open-source solutions like OpenPose as well as commercial
                        offerings like Cubemos and Nuitrack. This is a demo of a real-time 3D skeletal
                        tracking SDK integrated in Unity: since I used a depth camera (Intel Realsense D415), which
                        provides
                        an additional channel, there are fewer dropped detections than by using just
                        RGB information.</p>

                    <figure>
                        <img src='assets/silly_walks/demo_compressed.gif' alt='Unity Nuitrack 3d skeletal tracking demo'
                            class="center-silly" />
                        <figcaption style="font-size: 16px; text-align: center;" class="center-silly">
                            Even when indoors with good lighting, there are still limitations like camera
                            focus, connectivity, jitter, depth of field of sensor, number of skeletons detected, number
                            of joints detected
                            (max 24 for this model) and occlusion.

                        </figcaption>
                    </figure>

                    <p>Finally, it is silly walking time: I trained a <a
                            href="https://github.com/tensorflow/tfjs-models/tree/master/posenet"
                            target="_blank">PoseNet</a> model to
                        identify three poses: standing, raising
                        left knee and raising right knee. Data collection was done with a webcam and under 20 images for
                        each class. The result is shown in the following video, with probability scores (0-1) shown next
                        to predictions for the respective poses.
                    </p>

                    <figure>
                        <img src='assets/silly_walks/sillywalks_compressed.gif'
                            alt='Demo showing classification of left and right knee raises and standing still'
                            class="center-silly" />
                        <figcaption style="font-size: 16px; text-align: center;" class="center-silly">
                            While this classifier only predicts three atomic poses, it can be extended to recognise more
                            complicated ones. This can also be extended as a sequence classification problem, or perhaps
                            to "co-choreograph" new silly walks or actions.

                        </figcaption>
                    </figure>

                    <h2 class="section-heading">The verdict</h2>


                    <p>This quick experiment shows that tools can be quite easily integrated to measure and detect how
                        we walk, to varying degrees of success and silliness. But is there cause for concern around mass
                        surveillance based on how we walk?
                    </p>

                    <p>
                        To re-identify a person based solely on the way we walk is no small feat. Outside
                        a laboratory setting, gait identification is not as widely deployed as facial recognition. But
                        the possibility for identification becomes higher when combined with <a
                            href="https://www.research.manchester.ac.uk/portal/files/157901757/Multi_modality_sensor_fusion_for_gait_classification_using_deep_learning.pdf"
                            target="_blank">sensor data</a> (e.g. in smart
                        clothing or walking surfaces), as well as features alongside walking pose, which, in some
                        cases, can be intrusive and creepy if lacking informed consent over how data is being collected
                        and used.
                    </p>

                    <p>
                        But there are also scalable and useful applications. Pose estimation and related
                        research has been deployed in tools that are used in applications from <a
                            href="https://www.versovision.com/en" target="_blank">fall monitoring in
                            hospitals</a>, <a href="http://downloads.hindawi.com/journals/misy/2018/7381264.pdf"
                            target="_blank">identifying Parkinsonain gait</a>, and home-based <a
                            href="https://dl.acm.org/doi/10.1145/3316782.3316790" target="_blank">physical therapy</a>
                        to
                        the <a
                            href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Amit_Raj_SwapNet_Garment_Transfer_ECCV_2018_paper.pdf"
                            target="_blank">modelling of
                            clothes</a> in online shopping, correcting <a
                            href="https://link.springer.com/article/10.1007/s00521-019-04232-7" target="_blank">yoga
                            poses</a>, <a href="https://www.intelrealsense.com/interactive-art/"
                            target="_blank">touch-free
                            art</a>, and <a href="https://github.com/NVlabs/Dancing2Music" target="_blank">synthesizing
                            dance moves
                            from music</a>.
                    </p>
                    <p>
                        Perhaps our
                        best defence is more critical thinking around newer
                        technologies, prioritising ethics in the deployment of tools and funding of products, and
                        regulatory oversight.
                    </p>
                </div>
            </div>
        </div>

        <br>
    </article>

    <script src="js/jquery.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/clean-blog.min.js"></script>

</body>

</html>